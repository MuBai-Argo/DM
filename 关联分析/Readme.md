# 关联分析

Date:2021-09-07

Made By 纸 石头 紫阳花

______



[TOC]

## 基本概念

关联分析用于发现隐藏在大型数据集中有意义的联系，所发现的联系可以用关联规则或者==频繁项集==形式来表示。

1. 如何对关联特征进行挖掘
2. 如何对挖掘出的特征关联模式进行评估，以免产生虚假数据。

### 项集

设I为购物篮数据中所有项的集合，T时所有事务的集合，每个事务包含的项集都是I的子集。

### 关联规则

比如$\{牛奶， 尿布\} \to \{啤酒\}$,{牛奶，尿布，啤酒}作为样集

项集X与项集Y为不相交项集，则关联规则是二者之间的蕴含表达式。关联规则的强度可以通过==支持度==和==置信度==来进行度量。
$$
支持度：s(X \to Y) = \frac{\sigma(X\or Y)}{N}\\
置信度：c(X \to Y) = \frac{\sigma(X\or Y)}{\sigma(X)}\\
$$
N表示事务总数，$\sigma$表示对符合X或Y的事务进行计数。

支持度表示规则可以用于给定数据集的频繁程度，置信度表示Y在X的事务中出现的频繁程度。

置信度表示在$X\to Y$​中置信度越高，Y结果在包含X的事务中就越有可能出现，X、Y的关联性就越强。



### 关联规则发现

关联规则的发现与解释依赖于支持度与置信度的值，当两个值超过阈值minsup和minconf是，代表关联规则成立，X和Y事件之间具有关联性，且关联性相对大小可根据支持度和置信度进行排序。

大多数关联规则挖掘算法通常采用的一种策略是将关联规则挖掘任务分解为两个步骤：

1. 生成频繁项集：发现满足最小支持度阈值的所有项集
2. 产生规则从发现的频繁项集中提取所有高置信度的规则（强规则）

#### 频繁项集的生成

发现频繁项集的一种原始方法是确定格结构中各个候选项集的支持度计数。从而必须将每个候选项集与每个事务进行比较，若发现候选项集包含在事务中，则候选项集的支持度提升。

这种方法的计算复杂度非常大，可以通过==减少候选项集数目==和==减少比较次数==两种方式来降低频繁项集的计算复杂度。

##### 减少候选项集数目

使用先验原理（Apriori rules）绕过支持度计算，删除某些候选项集

##### 减少比较次数

使用更高级的数据结构或者存储候选数据项或者压缩数据集从而达到减少比较次数的目的



#### 先验原理

一个项集的支持度永远不会超过其子集（就像决策树的纯度，没有一个父结点的纯度会大于其子结点）如果一个项集是频繁的，则其所有子集一定也是频繁项集。若某项集不是频繁的，则其不存在超集是频繁的，可以直接对包含此项集的子图进行剪枝，这称为==基于支持度的剪枝方法==。

基于先验原理，可以利用先验算法（Apriori Algorithms）生成频繁项集。

##### Apriori Algorithms

Apriori Algorithms利用基于支持度的剪枝方法，系统性的控制候选项集指数增长。

Apriori Algorithms从除空以外最小项集开始由子集向超集逐层构建。

1. 初始时每个项都被看做1-项集，并对支持度进行计算。

2. 根据支持度阈值进行剪枝，抛弃无关的候选项集。

3. 利用 Apriori-gen Algorithms对尚存项集进行迭代：

    1. 将n个1-项集互相组合，生成$C^{2}_{n}$​个（完全的、必要的、不重复的）不同项集(2表示当前层数)。

        1. 蛮力方法生成新项集：
            蛮力方法把所有的项集组合作为新的项集然后利用剪枝一一除去。计算复杂度非常高
        2. $F_{k-1}\times F_{1}$方法：
            用其他频繁项来扩展每个频繁的(k-1)-项集
        3. $F_{k-1}\times F_{k-1}$方法：
            Apriori-gen Algorithms 合并一对频繁(k-1)项集，当且仅当其前k-2项都相同。

    2. 利用==支持度计数过程==计算新生成的项集的支持度，根据阈值进行剪枝（对项集所有真子集的支持率进行计算，若均大于阈值则无需剪除）（Question： 阈值如何进行确定比较合适）。
        支持度计数将每个**事务**与所有的**候选项**进行比较，并且更新包含在事务中的候选项的支持度计数。除此以外，还可以通过枚举每个事务所包含的项集，利用它们更新事务对应的候选项支持度。那些不与任何候选项对应的事务的子集可以忽略。

        > 在Apriori-gen Algorithms中，候选项被划分为不同的桶存放在Hash树中，在支持度计数期间，包含在事务中的项集也三列岛相应桶中。从而将事务中每个桶中，事务中的每个项集与同一桶中的所有候选项进行匹配

    3. 剪枝后尚存的候选项集就是频繁的2-项集。

4. 输出2-项集

5. 重复迭代，直到不再有新的频繁项集生成（频繁项集达到最大长度，可能包含所有项）。

算法需要的总的迭代次数是（频繁项集的最大长度+1）。

###### Apriori Algorithms计算复杂度影响因素

支持度阈值、项数（项数愈大，格结构深度愈大）、事务数、事务平均宽度、频繁1-项集的产生、候选的产生、Hash树、支持度计数



### 关联规则的生成

在获取了频繁项集的基础上，开始对关联规则进行提取，每个频繁k-项集能产生$2^k-2$个关联规则。

关联规则的提取方法：将项集Y划分成两个非空的子集X和Y-X，使得$X\to Y-X$满足置信度阈值。（由于规则由频繁项集中提取而成，故必然满足支持度阈值）。

计算关联规则的置信度不需要再次扫描事务数据集，而可以直接由频繁项集生成：
$$
置信度 = \frac{\sigma(频繁项集)}{\sigma(X)}
$$
当比较频繁项集Y参数的规则时，存在定理：

如果规则$X \to Y - X$不满足置信度阈值，则形如$X^{'} \to Y- X^{'}$也一定不满足置信度阈值，$X^{'}$是X的子集。

#### Apriori Algorithms提取关联规则

Apriori Algorithms逐层提取关联规则，每层对应于规则后件中的项数。

如$\{acd\}\to \{b\}和\{abd\}\to \{c\}$两个高置信度规则通过合并两个规则的后件后生成候选规则$\{ad\}\to\{bc\}$​。若任意低置信度结点存在则可以剪去其生成的整个子图。​

### 频繁项集的紧凑表示

#### 极大频繁集

极大频繁集的直接超集都并非频繁集。

极大频繁集提供了频繁项集的紧凑表示，可以导出所有频繁项集的最小的项集的集合。但同时，极大频繁集不包含子集的支持度信息。

#### 闭项集

闭项集提供了频繁项集的最小表示，该表示不丢失支持度信息，其任何直接超集都不具有与其相同的支持度计数。

#### 闭频繁项集

闭频繁项集是在闭项集的基础上其支持度大于或等于最小支持度阈值。



### 产生频繁项集的方法

Apriori方法使用先验原理对指数搜索区间进行剪枝，处理了组合爆炸带来的计算复杂度问题，然而因为在产生频繁项集时计算支持度需要多次扫描事务数据集，随着事务数据宽度的增加，Apriori方法的性能将显著下降。

故需要用一些其他方法对频繁项集进行计算。

#### 项集格遍历

概念上来说可以将频繁集的搜索看作对其表达的项集格进行遍历，算法使用的搜索策略将指明频繁项集产生过程中应当如何白能力格结构。

#### FP增长算法

FP增长算法通过使用一种称作==FP树==的紧凑数据结构，并直接从该数据结构中对频繁项集进行提取。

##### FP树

通过逐个读入事务，并将每个事务映射到FP树中的一条路径来构造，由于不同的事务可能有若干个相同的项，因此FP树的路径允许重叠，且重叠部分越多，FP树结构压缩的效果越好。于是便可以直接冲FP树种提取频繁项集。

###### FP树的构造方法

1. 起初FP树只包含一个根节点，用null进行标记。
2. 扫描数据集，确定每个项的支持度计数，并用阈值进行过滤，将频繁项按照支持度的递减顺序进行排列。
3. 二次扫描数据集，构建FP树，读入第一个事务后，利用事务中存在的项构造路径，并对事务进行编码，该路径上所有结点的频度计数为1.
4. 读入第二个事务后为其中项创建新的结点集并构造路径，该路径上频度计数也为1。
5. 当事务存在共同前缀并存在共同项时，两条路径相交，若存在则前缀结点频度计数+1
6. 继续直到所有事物都映射为FP树的一条路径。

FP树还包含一个连接相同项的指针列表，有助于快速访问树中的项。

###### FP增长算法的频繁项集的生成

FP树是一种自底而上的探索树。FP增长通过采用分治策略，从而发现以某个特定后缀结尾的所有频繁项集，并逐层对不同的后缀结尾进行频繁项集的提取。

1. 首先收集所有包含最底层某节点的所有路径，称之为前缀路径。
2. 由前缀路径，通过将于该结点相关联的支持度计数相加得到该结点的支持度计数。若大于支持度阈值则记为频繁项集。
3. 首先必须更新亲啊追路径上的支持度计数，因为某些计数包含那些不包含该结点的事务，将该前缀路径上的计数调整为1。
4. 层数上升，删除该结点并修建前缀路径。
5. 跟新研前缀路径的支持度计数之后，某些项可能不再是频繁项，则在之后的分析中忽略之。
6. 重复直到最后一个频繁项集。

FP增长算法的运行性能依赖于数据集的压缩因子，如果生成的条件FP树非常茂盛，则性能显著下降。

### 关联模式的评估

#### 兴趣度的客观度量

客观度量是一种评估关联模式质量的数据驱动方法，常常基于==相依表==中列出的频度计数来进行计算。

##### 支持度-置信度框架局限

现有的关联规则的挖掘算法需要使用支持度和置信度来出去没有意义的模式。

而支持度的缺点子啊与许多有意义的模式可能会由于支持度国小而被删去。置信度的缺陷在于该度量忽略了规则后件中项集的支持度。

##### 用于评估关联模式的客观度量

**兴趣因子：**

为了弥补置信度忽略规则后见中出现的项集的支持度的问题，使用称作==提升度==的度量：
$$
lift(A\to B) = \frac{c(A\to B)}{s(B)}
$$
从而对规则置信度和规则后件中项集的支持度之间的比例。

对于二元变量，提升度等价于==兴趣因子==定义为：
$$
I(A\to B) = \frac{s(A, B)}{s(A) \times s(B)} = \frac{Nf_{11}}{f_{1+}f_{+1}}
$$
兴趣因子比较模式的概率与统计独立假定下的极限概率。

对于相互独立的两个变量，基线频率为：
$$
\frac{f_{11}}{N} = \frac{f_{1+}}{N} \times \frac{f_{+1}}{N}\\
或等价为f_{11} =  \frac{f_{1+}f_{+1}}{N}\\
\frac{f_{11}}{N}时联合概率P(A, B)的估计\\
\frac{f_{1+}}{N} 和 \frac{f_{+1}}{N}分别是P(A)和P(B)的估计
$$
兴趣因子可作如下解释：
$$
I(A, B) \begin{cases} 
=1, & 如果A和B独立 \\
>1, & 如果A和B正相关\\
<1, & 如果A和B负相关\\
\end{cases}
$$
**相关分析**

对于连续变量，相关度用皮尔森相关系数定义。对于二元变量，相关度可以用$\phi$系数度量定义。

$\phi$系数定义为：
$$
\phi = \frac{f_{11}f_{00} - f_{01}f_{10}}{\sqrt{f_{1+}f_{+1}f_{0+}f_{+0}}}
$$
相关度从-1（完全负相关）到+1（完全正相关），若变量是统计独立的，则$\phi = 0$。

$\phi$系数把项在事务中同时出现和同时不出现看作同等重要，适用于分析对称的二元变量，且样本呢大小成比例变化时，它不能保持不变。

**IS度量**

IS度量专用于处理非对称二元变量，其定义为：
$$
IS(A, B) = \sqrt{I(A, B)\times s(A, B)} = \frac{s(A, B)}{\sqrt{s(A)s(B)}}
$$
当模式兴趣因子和模式支持度都很大时，IS往往也很大。

IS度量存在于置信度度量类似的问题，即使不相干或负相关的模式，IS度量仍然可能很大。

#### 客观度量的性质

##### 反演性

客观度量M在反演操作下是不变的，如果交换频度计数，它的值不会改变。

满足反演性的系数由$\phi$系数、几率、k和集体强度。这些度量不适合分析非对称的二元数据。

##### 零加性

像数据集中添加不相干数据的过程就是所谓零加操作，客观度量在零加操作下不变，则满足零加性。

满足零加性的客观度量包括IS度量和Jaccard度量。

##### 缩放不变性

客观变量在行列缩放操作下不变。

只有几率满足缩放不变性。

### 倾斜支持度分布的影响

具有倾斜支持度的数据集，其中大多数具有较低或者中等的频率，但是很少具有很高的频率。

对于倾斜支持度分布的数据集，在选择合适的支持度阈值上存在很大的困难。如果阈值太低，挖掘关联模式会非常困难，如果阈值过高，提取出来的关联模式数量会大幅提升，并且提取出大量高频率项和低频率项相关联的虚假模式，即==交叉支持==模式，由于其相关性太弱，这种模式大多是虚假的。

#### 交叉支持模式

交叉支持模式是一个项集$X = {i_1, i_2, ..., i_k}$, 其支持度比率：
$$
r(X) = \frac{min[s(i_1), s(i_2), ..., s(i_k)]}{max[s(i_1), s(i_2), ..., s(i_k)]}
$$
 小于用户指定的阈值$h_c$。

现有的度量都不足以消除交叉支持模式，置信度裁剪亦无济于事，因为交叉支持模式有时能产生高置信度的规则，所以很难用执行都度量区别从交叉支持模式中提取的规则。

但是可以通过检查由给定项集提取的最低置信度规则来检测交叉支持模式。

h置信度由于支持度的反单调性，h置信度度量的分子受限于频繁项集所有项中最小的支持度。即绝对小于或等于r(X)。因此，h置信度也一定小于阈值$h_c$。

使用h置信度不仅可以消除交叉支持模型，且其本身也是饭单调的，从而可以直接将其带入挖掘算法。并且h置信度能够确保项集中的项时强相联的，这种强相联模式又称为超团模式。

